{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Load the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "        We have taken a very small subset of data from Kaggle Loan defaulter competition for the purpose of the demo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "Demo for GIDs\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "DIR_PATH = \"C://Sapient//PYTHON//scikit-learn-master//examples//kaggle//competition//loan defaulter//data//\"\n",
      "TRAIN_FILE = DIR_PATH + \"sample_5000.csv\"\n",
      "\n",
      "X = pd.read_csv(TRAIN_FILE, sep=',', warn_bad_lines=True, error_bad_lines=True)\n",
      "\n",
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 129,
       "text": [
        "(5000, 772)"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature Engineering"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "        Explore the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 130,
       "text": [
        "Index([u'Unnamed: 0', u'id', u'f1', u'f2', u'f3', u'f4', u'f5', u'f6', u'f7', u'f8', u'f9', u'f10', u'f13', u'f14', u'f15', u'f16', u'f17', u'f18', u'f19', u'f20', u'f21', u'f22', u'f23', u'f24', u'f25', u'f26', u'f27', u'f28', u'f29', u'f30', u'f31', u'f32', u'f33', u'f34', u'f35', u'f36', u'f37', u'f38', u'f39', u'f40', u'f41', u'f42', u'f43', u'f44', u'f45', u'f46', u'f47', u'f48', u'f49', u'f50', u'f51', u'f52', u'f53', u'f54', u'f55', u'f56', u'f57', u'f58', u'f59', u'f60', u'f61', u'f62', u'f63', u'f64', u'f65', u'f66', u'f67', u'f68', u'f69', u'f70', u'f71', u'f72', u'f73', u'f74', u'f75', u'f76', u'f77', u'f78', u'f79', u'f80', u'f81', u'f82', u'f83', u'f84', u'f85', u'f86', u'f87', u'f88', u'f89', u'f90', u'f91', u'f92', u'f93', u'f94', u'f95', u'f96', u'f97', u'f98', u'f99', u'f100', u'f101', u'f102', u'f103', u'f104', u'f105', u'f106', u'f107', u'f108', u'f109', u'f110', u'f111', u'f112', u'f113', u'f114', u'f115', u'f116', u'f117', u'f118', u'f119', u'f120', u'f121', u'f122', u'f123', u'f124', u'f125', u'f126', u'f127', u'f128', u'f129', u'f130', u'f131', u'f132', u'f133', u'f134', u'f135', u'f136', u'f137', u'f138', u'f139', u'f140', u'f141', u'f142', u'f143', u'f144', u'f145', u'f146', u'f147', u'f148', u'f149', u'f150', u'f151', u'f152', u'f153', u'f154', u'f155', u'f156', u'f157', u'f158', u'f159', u'f160', u'f161', u'f162', u'f163', u'f164', u'f165', u'f166', u'f167', u'f168', u'f169', u'f170', u'f171', u'f172', u'f173', u'f174', u'f175', u'f176', u'f177', u'f178', u'f179', u'f180', u'f181', u'f182', u'f183', u'f184', u'f185', u'f186', u'f187', u'f188', u'f189', u'f190', u'f191', u'f192', u'f193', u'f194', u'f195', u'f196', u'f197', u'f198', u'f199', u'f200', u'f201', u'f202', u'f203', u'f204', u'f205', u'f206', u'f207', u'f208', u'f209', u'f210', u'f211', u'f212', u'f213', u'f214', u'f215', u'f216', u'f217', u'f218', u'f219', u'f220', u'f221', u'f222', u'f223', u'f224', u'f225', u'f226', u'f227', u'f228', u'f229', u'f230', u'f231', u'f232', u'f233', u'f234', u'f235', u'f236', u'f237', u'f238', u'f239', u'f240', u'f241', u'f242', u'f243', u'f244', u'f245', u'f246', u'f247', u'f248', u'f249', u'f250', u'f251', u'f252', u'f253', u'f254', u'f255', u'f256', u'f257', u'f258', u'f259', u'f260', u'f261', u'f262', u'f263', u'f264', u'f265', u'f266', u'f267', u'f268', u'f269', u'f270', u'f271', u'f272', u'f273', u'f274', u'f275', u'f276', u'f277', u'f278', u'f279', u'f280', u'f281', u'f282', u'f283', u'f284', u'f285', u'f286', u'f287', u'f288', u'f289', u'f290', u'f291', u'f292', u'f293', u'f294', u'f295', u'f296', u'f297', u'f298', u'f299', u'f300', u'f301', u'f302', u'f303', u'f304', u'f305', u'f306', u'f307', u'f308', u'f309', u'f310', u'f311', u'f312', u'f313', u'f314', u'f315', u'f316', u'f317', u'f318', u'f319', u'f320', u'f321', u'f322', u'f323', u'f324', u'f325', u'f326', u'f327', u'f328', u'f329', u'f330', u'f331', u'f332', u'f333', u'f334', u'f335', u'f336', u'f337', u'f338', u'f339', u'f340', u'f341', u'f342', u'f343', u'f344', u'f345', u'f346', u'f347', u'f348', u'f349', u'f350', u'f351', u'f352', u'f353', u'f354', u'f355', u'f356', u'f357', u'f358', u'f359', u'f360', u'f361', u'f362', u'f363', u'f364', u'f365', u'f366', u'f367', u'f368', u'f369', u'f370', u'f371', u'f372', u'f373', u'f374', u'f375', u'f376', u'f377', u'f378', u'f379', u'f380', u'f381', u'f382', u'f383', u'f384', u'f385', u'f386', u'f387', u'f388', u'f389', u'f390', u'f391', u'f392', u'f393', u'f394', u'f395', u'f396', u'f397', u'f398', u'f399', u'f400', u'f401', u'f402', u'f403', u'f404', u'f405', u'f406', u'f407', u'f408', u'f409', u'f410', u'f411', u'f412', u'f413', u'f414', u'f415', u'f416', u'f417', u'f418', u'f419', u'f420', u'f421', u'f422', u'f423', u'f424', u'f425', u'f426', u'f427', u'f428', u'f429', u'f430', u'f431', u'f432', u'f433', u'f434', u'f435', u'f436', u'f437', u'f438', u'f439', u'f440', u'f441', u'f442', u'f443', u'f444', u'f445', u'f446', u'f447', u'f448', u'f449', u'f450', u'f451', u'f452', u'f453', u'f454', u'f455', u'f456', u'f457', u'f458', u'f459', u'f460', u'f461', u'f462', u'f463', u'f464', u'f467', u'f468', u'f469', u'f470', u'f471', u'f472', u'f475', u'f476', u'f477', u'f478', u'f479', u'f480', u'f481', u'f482', u'f483', u'f484', u'f485', u'f486', u'f487', u'f488', u'f489', u'f490', u'f491', u'f492', u'f493', u'f494', u'f495', u'f496', u'f497', u'f498', u'f499', u'f500', u'f501', u'f502', u'f503', u'f504', u'f505', u'f506', u'f507', u'f508', u'f509', u'f510', u'f511', u'f512', u'f513', u'f514', u'f515', u'f516', u'f517', u'f518', u'f519', u'f520', u'f521', u'f522', u'f523', u'f524', u'f525', u'f526', u'f527', u'f528', u'f529', u'f530', u'f531', u'f532', u'f533', u'f534', u'f535', u'f536', u'f537', u'f538', u'f539', u'f540', u'f541', u'f542', u'f543', u'f544', u'f545', u'f546', u'f547', u'f548', u'f549', u'f550', u'f551', u'f552', u'f553', u'f554', u'f555', u'f556', u'f557', u'f558', u'f559', u'f560', u'f561', u'f562', u'f563', u'f564', u'f565', u'f566', u'f567', u'f568', u'f569', u'f570', u'f571', u'f572', u'f573', u'f574', u'f575', u'f576', u'f577', u'f578', u'f579', u'f580', u'f581', u'f582', u'f583', u'f584', u'f585', u'f586', u'f587', u'f588', u'f589', u'f590', u'f591', u'f592', u'f593', u'f594', u'f595', u'f596', u'f597', u'f598', u'f599', u'f600', u'f601', u'f604', u'f606', u'f607', u'f608', u'f609', u'f610', u'f611', u'f612', u'f613', u'f614', u'f615', u'f616', u'f617', u'f618', u'f619', u'f620', u'f621', u'f622', u'f623', u'f624', u'f625', u'f626', u'f627', u'f628', u'f629', u'f630', u'f631', u'f632', u'f633', u'f634', u'f635', u'f636', u'f637', u'f638', u'f639', u'f640', u'f641', u'f642', u'f643', u'f644', u'f645', u'f646', u'f647', u'f648', u'f649', u'f650', u'f651', u'f652', u'f653', u'f654', u'f655', u'f656', u'f657', u'f658', u'f659', u'f660', u'f661', u'f662', u'f663', u'f664', u'f665', u'f666', u'f667', u'f668', u'f669', u'f670', u'f671', u'f672', u'f673', u'f674', u'f675', u'f676', u'f677', u'f678', u'f679', u'f680', u'f681', u'f682', u'f683', u'f684', u'f685', u'f686', u'f687', u'f688', u'f689', u'f690', u'f691', u'f692', u'f693', u'f694', u'f695', u'f696', u'f697', u'f698', u'f699', u'f700', u'f701', u'f702', u'f703', u'f704', u'f705', u'f706', u'f707', u'f708', u'f709', u'f710', u'f711', u'f712', u'f713', u'f714', u'f715', u'f716', u'f717', u'f718', u'f719', u'f720', u'f721', u'f722', u'f723', u'f724', u'f725', u'f726', u'f727', u'f728', u'f729', u'f730', u'f731', u'f732', u'f733', u'f734', u'f735', u'f736', u'f737', u'f738', u'f739', u'f740', u'f741', u'f742', u'f743', u'f744', u'f745', u'f746', u'f747', u'f748', u'f749', u'f750', u'f751', u'f752', u'f753', u'f754', u'f755', u'f756', u'f757', u'f758', u'f759', u'f760', u'f761', u'f762', u'f763', u'f764', u'f765', u'f766', u'f767', u'f768', u'f769', u'f770', u'f771', u'f772', u'f773', u'f774', u'f775', u'f776', u'f777', u'f778', u'loss'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "array([[0L, 1L, 126L, ..., 0L, 5L, 0L],\n",
        "       [1L, 2L, 121L, ..., 0L, 5L, 0L],\n",
        "       [2L, 3L, 126L, ..., 0L, 5L, 0L],\n",
        "       ..., \n",
        "       [4997L, 4998L, 128L, ..., 0L, 513L, 0L],\n",
        "       [4998L, 4999L, 133L, ..., 0L, 513L, 0L],\n",
        "       [4999L, 5000L, 130L, ..., 0L, 513L, 0L]], dtype=object)"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X['f768'].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "        explore the 'loss' column"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.loss[X.loss > 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 132,
       "text": [
        "7       1\n",
        "16     16\n",
        "26     19\n",
        "50      4\n",
        "64     11\n",
        "68      1\n",
        "77     11\n",
        "87     21\n",
        "90      4\n",
        "111     4\n",
        "120     2\n",
        "127     2\n",
        "136     9\n",
        "144     5\n",
        "161     2\n",
        "...\n",
        "4851     5\n",
        "4865     3\n",
        "4866     7\n",
        "4882    17\n",
        "4892     1\n",
        "4896    15\n",
        "4918     4\n",
        "4927    11\n",
        "4941    24\n",
        "4952     1\n",
        "4954     1\n",
        "4962     2\n",
        "4965     4\n",
        "4979     7\n",
        "4984     7\n",
        "Name: loss, Length: 432, dtype: int64"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.asarray(X.values, dtype = float)\n",
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "(5000L, 772L)"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where(np.isnan(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Imputation of Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import Imputer\n",
      "imputer = Imputer(strategy='mean')\n",
      "\n",
      "X = imputer.fit_transform(X)\n",
      "\n",
      "print \"Since we have replaced nan with median, the following should not return a null array\"\n",
      "np.where(np.isnan(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Since we have replaced nan with median, the following should not return a null array\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 136,
       "text": [
        "(array([], dtype=int64), array([], dtype=int64))"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Separate the features and the labels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "\n",
      "print \"Let's extract label from the data\"\n",
      "labels = np.asarray(X[:,-1], dtype = float)\n",
      "print \"Now Let's extract the features\"\n",
      "data = np.asarray(X[:,1:-2], dtype = float)\n",
      "\n",
      "print data[10][16]\n",
      "data = preprocessing.scale(data)\n",
      "print data[10][16]\n",
      "print data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are interested in the classification problem. Whether the customer will default or not.\n",
      "\n",
      "So if loss contains a number more than zero (0), it means that the customer defaulted"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels = np.asarray(map(int,labels))\n",
      "\n",
      "bin_labels = []\n",
      "    \n",
      "for i in range(len(labels)):\n",
      "    if labels[i] > 0:\n",
      "        bin_labels.append(1)\n",
      "    else:\n",
      "        bin_labels.append(0)\n",
      "        \n",
      "bin_labels = np.asarray(bin_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "a_train, a_test, b_train, b_test = train_test_split(data, bin_labels, test_size=0.2, random_state=42)\n",
      "\n",
      "b_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Apply Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To train and build a model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from time import time\n",
      "\n",
      "lg = LogisticRegression()\n",
      "\n",
      "t0 = time()\n",
      "print \"Training: \", 'Logistic Regression ', 20 * '_'        \n",
      "lg.fit(a_train, b_train)\n",
      "print \"Training time: %0.3fs\" % (time() - t0)\n",
      "#pred = model.predict(a_test)\n",
      "print \"Logistic Regression SCORE:\", lg.score(a_test, b_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training:  Logistic Regression  ____________________\n",
        "Training time: 6.040s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Logistic Regression SCORE: 0.896\n"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "        Find the accuracy of other classifiers\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "models = { 'LogisticRegression': LogisticRegression(), \n",
      "            'SVC': SVC(),\n",
      "            #'GradientBoostingClassifier': GradientBoostingClassifier(), #takes very high training time 160+ sec\n",
      "            'KNN': KNeighborsClassifier(),\n",
      "            'GaussianNB': GaussianNB()    \n",
      "         }\n",
      "\n",
      "for i, (model_name, model) in enumerate(models.iteritems()):\n",
      "        t0 = time()\n",
      "        print \"Training: \", model_name, 20 * '_'        \n",
      "        model.fit(a_train, b_train)\n",
      "        print \"Training time: %0.3fs\" % (time() - t0)\n",
      "        #pred = model.predict(a_test)\n",
      "        print model_name, \"SCORE:\", model.score(a_test, b_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "        Decision Tree Classifier:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "tree_clf = DecisionTreeClassifier()\n",
      "\n",
      "t0 = time()\n",
      "print \"Training: Decision Tree Classifier\", 20 * '_'        \n",
      "tree_clf.fit(a_train, b_train)\n",
      "print \"Training time: %0.3fs\" % (time() - t0)\n",
      "#pred = model.predict(a_test)\n",
      "print \" Decision Tree Classifier SCORE:\", tree_clf.score(a_test, b_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training: Decision Tree Classifier ____________________\n",
        "Training time: 9.068s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Decision Tree Classifier SCORE: 0.836\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree_clf.score(a_train, b_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ensemble Method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "predictions = []\n",
      "\n",
      "pred1 = tree_clf.predict_proba(a_test)\n",
      "predictions.append(array(pred1[:,1]))\n",
      "pred2 = lg.predict_proba(a_test)\n",
      "predictions.append(array(pred2[:,1]))\n",
      "\n",
      "\n",
      "final_proba = predictions[0] * 0.4 + predictions[1] * 0.6\n",
      "final_prediction = []\n",
      "\n",
      "print final_proba[1]\n",
      "\n",
      "\n",
      "for i in range(len(final_proba)):\n",
      "    if final_proba[i] > 0.667:\n",
      "        final_prediction.append(1)\n",
      "    else:\n",
      "        final_prediction.append(0)\n",
      "       \n",
      "print \"Classification Score of Ensembling two models\", metrics.accuracy_score(b_test, final_prediction)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0871676866588\n",
        "Classification Score of Ensembling two models 0.904\n"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature Selection:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "let's try to get the most important features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "lsvc = LinearSVC(penalty=\"l1\", dual=False)\n",
      "lsvc.fit(a_train, b_train)\n",
      "a_train_trfmd = lsvc.transform(a_train)\n",
      "a_test_trfmd =  lsvc.transform(a_test)\n",
      "\n",
      "a_test_trfmd.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc = SVC()\n",
      "\n",
      "t0 = time()\n",
      "print \"Training: SVC\", 20 * '_'        \n",
      "svc.fit(a_train_trfmd, b_train)\n",
      "print \"Training time: %0.3fs\" % (time() - t0)\n",
      "print \"SVC SCORE:\", svc.score(a_test_trfmd, b_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Hyperparameter estimation: Grid Search"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import grid_search\n",
      "\n",
      "parameters = {\n",
      "              'C': np.logspace(-2, 2, 10) #,\n",
      "              #'kernel': ['linear', 'rbf', 'sigmoid']\n",
      "              }\n",
      "    \n",
      "clf = grid_search.GridSearchCV(svc, parameters, cv=2)\n",
      "clf.fit(a_train_trfmd, b_train)\n",
      "\n",
      "print 'grid_scores_', clf.grid_scores_\n",
      "#print 'best_estimator_', clf.best_estimator_\n",
      "print 'best_score_', clf.best_score_\n",
      "print 'best_params_', clf.best_params_\n",
      "\n",
      "# best_params_ {'kernel': 'rbf', 'C': 0.01}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = [g[1] for g in clf.grid_scores_]\n",
      "plt.semilogx(parameters['C'], scores);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc = SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0, \\\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None, \\\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "print svc.fit(a_train_trfmd, b_train).score(a_test_trfmd, b_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y_pred = svc.predict(a_test_trfmd)\n",
      "\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(metrics.confusion_matrix(y, y_pred),\n",
      "               cmap=plt.cm.binary, interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.xlabel('true value')\n",
      "    plt.ylabel('predicted value')\n",
      "    \n",
      "print \"Classification Accuracy Score:\", metrics.accuracy_score(b_test, y_pred)\n",
      "\n",
      "plot_confusion_matrix(b_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "# Let's do a 5-fold cross-validation of the SVC estimator\n",
      "print cross_val_score(SVC(C=0.01, kernel='rbf'), data, labels, cv=5, scoring='precision')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Visualizing the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reduce the dimension of the training set by using Principal Component Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "\n",
      "pca_X = pca.fit_transform(a_train_trfmd)\n",
      "\n",
      "pca_X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "\n",
      "import pylab as pl\n",
      "\n",
      "pl.figure(2, figsize=(8, 6))\n",
      "#pl.clf()\n",
      "\n",
      "# Plot the training points from PCA\n",
      "pl.scatter(pca_X[:, 0], pca_X[:, 1], c=b_train, cmap=pl.cm.Paired)\n",
      "pl.xlabel('X-component')\n",
      "pl.ylabel('Y-component')\n",
      "\n",
      "#pl.xlim(x_min, x_max)\n",
      "#pl.ylim(y_min, y_max)\n",
      "#pl.xticks(())\n",
      "#pl.yticks(())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ch2 = SelectKBest(chi2, k=100)\n",
      "#chi_train_trfmd = ch2.fit_transform(a_train, b_train)\n",
      "#chi_test_trfmd = ch2.transform(a_test)\n",
      "\n",
      "#a_test_trfmd.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import  scipy.stats as stats\n",
      "\n",
      "import cPickle\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.colors import ListedColormap\n",
      "\n",
      "# utility function to plot the decision surface\n",
      "def plot_surface(est, x_1, x_2, ax=None, threshold=0.0, contourf=False):\n",
      "    \"\"\"Plots the decision surface of ``est`` on features ``x1`` and ``x2``. \"\"\"\n",
      "    xx1, xx2 = np.meshgrid(np.linspace(x_1.min(), x_1.max(), 100), \n",
      "                           np.linspace(x_2.min(), x_2.max(), 100))\n",
      "    # plot the hyperplane by evaluating the parameters on the grid\n",
      "    X_pred = np.c_[xx1.ravel(), xx2.ravel()]  # convert 2d grid into seq of points\n",
      "    if hasattr(est, 'predict_proba'):  # check if ``est`` supports probabilities\n",
      "        # take probability of positive class\n",
      "        pred = est.predict_proba(X_pred)[:, 1]\n",
      "    else:\n",
      "        pred = est.predict(X_pred)\n",
      "    Z = pred.reshape((100, 100))  # reshape seq to grid\n",
      "    if ax is None:\n",
      "        ax = plt.gca()\n",
      "    # plot line via contour plot\n",
      "\n",
      "    if contourf:\n",
      "        ax.contourf(xx1, xx2, Z, levels=np.linspace(0, 1.0, 10), cmap=plt.cm.RdBu, alpha=0.6)\n",
      "    ax.contour(xx1, xx2, Z, levels=[threshold], colors='black')\n",
      "    ax.set_xlim((x_1.min(), x_1.max()))\n",
      "    ax.set_ylim((x_2.min(), x_2.max()))\n",
      "\n",
      "\n",
      "    \n",
      "def plot_datasets(est=None):\n",
      "    \"\"\"Plotsthe decision surface of ``est`` on each of the three datasets. \"\"\"\n",
      "    fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
      "    for (name, ds), ax in zip(datasets.iteritems(), axes):\n",
      "        X_train = ds['X_train']\n",
      "        y_train = ds['y_train']\n",
      "        X_test = ds['X_test']\n",
      "        y_test = ds['y_test']\n",
      "\n",
      "        # plot test lighter than training\n",
      "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
      "        # Plot the training points\n",
      "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
      "        # and testing points\n",
      "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
      "        # plot limits\n",
      "        ax.set_xlim(X_train[:, 0].min(), X_train[:, 0].max())\n",
      "        ax.set_ylim(X_train[:, 1].min(), X_train[:, 1].max())\n",
      "        # no ticks\n",
      "        ax.set_xticks(())\n",
      "        ax.set_yticks(())\n",
      "        ax.set_ylabel('$x_1$')\n",
      "        ax.set_xlabel('$x_0$')\n",
      "        ax.set_title(name)\n",
      "        if est is not None:\n",
      "            est.fit(X_train, y_train)\n",
      "            plot_surface(est, X_train[:, 0], X_train[:, 1], ax=ax, threshold=0.5, contourf=True)\n",
      "            err = (y_test != est.predict(X_test)).mean()\n",
      "            ax.text(0.88, 0.02, '%.2f' % err, transform=ax.transAxes)\n",
      "\n",
      "    fig.subplots_adjust(left=.02, right=.98)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}